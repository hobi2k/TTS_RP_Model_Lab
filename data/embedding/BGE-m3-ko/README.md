---
language:
- ko
- en
library_name: sentence-transformers
metrics:
- cosine_accuracy@1
- cosine_accuracy@3
- cosine_accuracy@5
- cosine_accuracy@10
- cosine_precision@1
- cosine_precision@3
- cosine_precision@5
- cosine_precision@10
- cosine_recall@1
- cosine_recall@3
- cosine_recall@5
- cosine_recall@10
- cosine_ndcg@10
- cosine_mrr@10
- cosine_map@100
- dot_accuracy@1
- dot_accuracy@3
- dot_accuracy@5
- dot_accuracy@10
- dot_precision@1
- dot_precision@3
- dot_precision@5
- dot_precision@10
- dot_recall@1
- dot_recall@3
- dot_recall@5
- dot_recall@10
- dot_ndcg@10
- dot_mrr@10
- dot_map@100
pipeline_tag: sentence-similarity
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
widget:
- source_sentence: ëŒ€í•œì§€ì ê³µì‚¬ ê´€ê³„ìëŠ” "ì˜¤ëœ ì§„í†µ ëì— ì§€ì ì¬ì¡°ì‚¬ì‚¬ì—…ì„ ì¶”ì§„í•˜ê²Œ ë¼ ê¸°ì˜ë‹¤"ë©´ì„œë„ ë­ë¼ê³  ë§í–ˆì–´?
  sentences:
  - >-
    2018 í‰ì°½ ë™ê³„ì˜¬ë¦¼í”½ì´ ê°œë§‰í•˜ê¸° ì „ 'ê³µê³µì˜ ì 'ì€ ì˜í•˜ 10ë„ë¥¼ ë„˜ëŠ” ì¶”ìœ„ì˜€ë‹¤. ê°œë§‰ì„ ì¦ˆìŒí•´ ì¶”ìœ„ëŠ” ì¡°ê¸ˆ ìˆ˜ê·¸ëŸ¬ë“œëŠ”ê°€ ì‹¶ë”ë‹ˆ
    ë°”ëŒì´ ë©ˆì¶”ì§€ ì•Šì•„ ëŒ€íšŒ 2ì¼ ì°¨ë¶€í„° ê²½ê¸°ê°€ ì‡ë‹¬ì•„ ì—°ê¸°Â·ì·¨ì†Œëë‹¤.

    ì˜¬ë¦¼í”½ ì¡°ì§ìœ„ì›íšŒì™€ êµ­ì œìŠ¤í‚¤ì—°ë§¹(FIS)ì€ 11ì¼ ì˜¤ì „ 11ì‹œ ì •ì„  ì•ŒíŒŒì¸ ê²½ê¸°ì¥ì—ì„œ ì—´ë¦´ ì˜ˆì •ì´ë˜ ì•ŒíŒŒì¸ ìŠ¤í‚¤ ë‚¨ì í™œê°• ê²½ê¸°ë¥¼
    ê°•í’ìœ¼ë¡œ ì—°ê¸°í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. FISëŠ” â€œê°•í’ì´ ê²½ê¸°ì¥ì— í•˜ë£¨ ì¢…ì¼ ê³„ì† ë¶ˆ ê²ƒìœ¼ë¡œ ì „ë§ë¼ ì¼ì •ì„ ì—°ê¸°í–ˆë‹¤â€ê³  ë°í˜”ë‹¤. ì¡°ì§ìœ„ëŠ”
    ì—°ê¸°ëœ ë‚¨ì í™œê°• ê²½ê¸°ë¥¼ ì˜¤ëŠ” 15ì¼ ì˜¤ì „ 11ì‹œì— ì¹˜ë¥´ê³ , ì´ ì‹œê°„ëŒ€ì— ì›ë˜ ì—´ë¦´ ì˜ˆì •ì´ë˜ ë‚¨ì ìŠˆí¼ëŒ€íšŒì „ ê²½ê¸° ì‹œê°„ì„ í•˜ë£¨ ë’¤ì¸
    16ì¼ ì˜¤ì „ 11ì‹œë¡œ ìˆœì—°í•˜ê¸°ë¡œ í–ˆë‹¤.

    ì´ì–´ ì´ë‚  ì˜¤í›„ 1ì‹œ30ë¶„ë¶€í„° ì—´ë¦´ ì˜ˆì •ì´ë˜ ìŠ¤ë…¸ë³´ë“œ ì—¬ì ìŠ¬ë¡œí”„ìŠ¤íƒ€ì¼ ì˜ˆì„  ê²½ê¸°ëŠ” ì—°ê¸°ë¥¼ ê±°ë“­í•˜ë‹¤ ì·¨ì†Œëë‹¤. ì¡°ì§ìœ„ëŠ” ì˜ˆì„  ì—†ì´ ë‹¤ìŒ
    ë‚  ê²°ì„ ì—ì„œ ì°¸ê°€ì 27ëª…ì´ í•œë²ˆì— ê²½ê¸°í•´ ìˆœìœ„ë¥¼ ê°€ë¦¬ê¸°ë¡œ í–ˆë‹¤.

    ê°•í’ì´ ê²½ê¸° ì§„í–‰ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë€ ì˜ˆìƒì€ ëŒ€íšŒ ì „ë¶€í„° ìˆì—ˆë‹¤. ì˜¬ë¦¼í”½ ëŒ€íšŒ ìŠ¬ë¡œí”„ê°€ ì„¤ì¹˜ëœ ì •ì„ Â·ìš©í‰ ì•ŒíŒŒì¸ ê²½ê¸°ì¥ê³¼ íœ˜ë‹‰ìŠ¤ ìŠ¤ë…¸
    ê²½ê¸°ì¥ì€ ìŠ¬ë¡œí”„ ìƒë‹¨ë¶€ì˜ í•´ë°œê³ ë„ê°€ 900mê°€ ë„˜ëŠ”ë‹¤. ì„ì¥í˜¸ ì¡°ì§ìœ„ ê¸°ìƒê¸°í›„íŒ€ì¥ì€ â€œì•ŒíŒŒì¸ ìŠ¤í‚¤ëŠ” ìƒë‹¨ë¶€ì— ê°•í•œ ë°”ëŒì´ ë¶ˆë©´, ì„ ìˆ˜ë“¤ì„
    ì‹¤ì–´ë‚˜ë¥´ëŠ” ê³¤ëŒë¼ë¥¼ ì›€ì§ì´ê¸° ì–´ë µë‹¤â€ë©° â€œìŠ¤ë…¸ë³´ë“œë‚˜ í”„ë¦¬ìŠ¤íƒ€ì¼ ìŠ¤í‚¤ëŠ” ìˆœê°„ì ì¸ ëŒí’ì´ ë¶ˆ ë•Œ ì„ ìˆ˜ë“¤ì´ ë‹¤ì¹  ê°€ëŠ¥ì„±ë„ ìˆë‹¤â€ê³  ë§í–ˆë‹¤.

    ë°”ëŒì´ ê²½ê¸°ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì•Œê¸°ì— ì¡°ì§ìœ„ë„ ê°•í’ì„ ë¹„ë¡¯í•œ 5ê°€ì§€ ê¸´ê¸‰ ê¸°ìƒ ìƒí™©ì„ ê°€ì •í•´ ê²½ê¸° ìš´ì˜ ë§¤ë‰´ì–¼ì„ ë§Œë“¤ì—ˆë‹¤. ì´ë‚  ê²½ê¸°
    ì·¨ì†Œë„ ë§¤ë‰´ì–¼ì— ë”°ë¥¸ ì¡°ì¹˜ì˜€ë‹¤. ì„ íŒ€ì¥ì€ â€œ12~13ì¼ ë°”ëŒì´ ì¦ì•„ë“¤ë‹¤ê°€ 14ì¼ì— ë‹¤ì‹œ ê°•í’ì´ ë¶ˆê² ì§€ë§Œ, 15ì¼ë¶€í„°ëŠ” ë‹¤ì‹œ ì¦ì•„ë“¤
    ê²ƒìœ¼ë¡œ ë³´ê³  ìˆë‹¤â€ë©° â€œí–¥í›„ ê°•í’ìœ¼ë¡œ ê²½ê¸°ê°€ ì—°ê¸°ë¼ë„ ì˜¬ë¦¼í”½ íë§‰ ì „ ìµœëŒ€í•œ ëª¨ë“  ê²½ê¸°ë¥¼ ëë‚´ë ¤ í•˜ê³  ìˆë‹¤â€ê³  í–ˆë‹¤. ë‹¤ë§Œ ê²½ê¸° ì¼ì •ì´
    ë°”ë€Œë©´ ì°¸ê°€ ì„ ìˆ˜ë“¤ê³¼ ì½”ì¹­ìŠ¤íƒœí”„ê°€ ì–´ë–»ê²Œ ì»¨ë””ì…˜ì„ ì¡°ì ˆí•˜ë©° ê²½ê¸°ë¥¼ ì¤€ë¹„í• ì§€ ê¹Šì€ ê³ ë¯¼ì— ë¹ ì§ˆ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
  - >-
    ì§€ì ë„ë©´ê³¼ ì‹¤ì œ ê²½ê³„ê°€ ë§ì§€ ì•ŠëŠ” 'ì§€ì ë¶ˆë¶€í•©ì§€'ì— ëŒ€í•œ ì¬ì¡°ì‚¬ê°€ ì‹¤ì‹œëœë‹¤. êµ­í† í•´ì–‘ë¶€ëŠ” ì§€ì ë„ìƒ ê²½ê³„ì™€ ì‹¤ì œ ê²½ê³„ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”
    ì§€ì ë¶ˆë¶€í•©ì§€ì— ëŒ€í•´ 2030ë…„ê¹Œì§€ ì§€ì ì¬ì¡°ì‚¬ë¥¼ ì¶”ì§„í•œë‹¤ê³  ì§€ë‚œë‹¬ 30ì¼ ë°í˜”ë‹¤. ì´ì™€ ê´€ë ¨ ê¹€ê¸°í˜„ ì˜ì›ì´ ëŒ€í‘œë°œì˜í•œ ì§€ì ì¬ì¡°ì‚¬íŠ¹ë³„ë²•ì•ˆì´
    ì´ë‚  êµ­íšŒ ìƒì„ìœ„ë¥¼ í†µê³¼í–ˆë‹¤. ì§€ì ë¶ˆë¶€í•©ì§€ëŠ” ê²½ê³„ë¶„ìŸê³¼ ë¯¼ì›ì˜ ëŒ€ìƒì´ ë˜ê³  ìˆëŠ”ë°, í˜„ì¬ ì „ì²´ í•„ì§€ì˜ ì•½ 15%(554ë§Œí•„ì§€)ì— ì´ë¥¼
    ê²ƒìœ¼ë¡œ ì¶”ì •ëœë‹¤. íŠ¹íˆ ìƒë‹¹ìˆ˜ëŠ” ì§€ì ì¸¡ëŸ‰ì´ ë¶ˆê°€ëŠ¥í•´ ì†Œìœ ê¶Œ ì´ì „ì´ë‚˜ ê±´ì¶•í–‰ìœ„ ë“± ì¬ì‚°ê¶Œ í–‰ì‚¬ê°€ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì œí•œë°›ê³  ìˆì–´ ì¡°ì •ì´ ì‹œê¸‰í•œ
    ìƒí™©ì´ë‹¤. ì´ì— ë”°ë¼ 1995ë…„ ì§€ì ì¬ì¡°ì‚¬ì‚¬ì—…ì¶”ì§„ ê¸°ë³¸ê³„íšì´ ìˆ˜ë¦½ë˜ê³ , ì´ë“¬í•´ ì§€ì ì¬ì¡°ì‚¬íŠ¹ë³„ë²•ì´ ì…ë²•ì˜ˆê³ ëì§€ë§Œ ê´€ë ¨ ë¶€ì²˜ë“¤ì˜ ë°˜ëŒ€ë¡œ
    ë¬´ì‚°ëë‹¤. ì´í›„ 2000ë…„ ë‹¤ì‹œ ì¬ì¡°ì‚¬ì‚¬ì—… ê¸°ë³¸ê³„íšì´ ìˆ˜ë¦½ë˜ê³ , 2006ë…„ í† ì§€ì¡°ì‚¬íŠ¹ë³„ë²•ì•ˆì´ ì œì¶œëìœ¼ë‚˜ ì„±ì‚¬ë˜ì§€ ëª»í•œ ì±„ ì˜¤ëŠ˜ì— ì´ë¥´ê³ 
    ìˆë‹¤. ì§€ì ë¶ˆë¶€í•©ì§€ëŠ” 100ë…„ ì „ ë‚™í›„ëœ ê¸°ìˆ ë¡œ ë§Œë“  ì¢…ì´ì§€ì ì„ ê³„ì† ì‚¬ìš©í•˜ë©´ì„œ ì¢…ì´ë„ë©´ì˜ ì‹ ì¶•, ê²½ê³„ì„ ì˜ êµµê¸°, ê°œì¸ì˜¤ì°¨ ë“±ìœ¼ë¡œ
    ìƒê²¨ë‚¬ë‹¤. ë˜ ëŒ€ì¥ì´ í† ì§€Â·ì„ì•¼ëŒ€ì¥ìœ¼ë¡œ ì´ì›í™”ë¼ ìˆê³ , ë„ë©´ë„ 7ì¢…ì˜ ì¶•ì²™ìœ¼ë¡œ ë“±ë¡ëœ ê²ƒë„ ì›ì¸ìœ¼ë¡œ ê¼½íŒë‹¤. ì¼ë¡€ë¡œ 1:1200 ì¶•ì²™ì˜
    ì••êµ¬ì •ë™ ëŒ€ì§€(280ã¡, 1000ë§Œì›/ã¡)ì˜ ê²½ìš° ì§€ì ë„ìƒ ê²½ê³„ê°€ 0.8mm ì˜¤ì°¨ê°€ ë‚˜ë©´ ì‹¤ì œ ë©´ì ì—ì„  27ã¡ì˜ ì°¨ì´ê°€ ë°œìƒ, ì•½
    2ì–µ7000ë§Œì›ì˜ ë•…ê°’ì´ ì°¨ì´ë‚˜ê²Œ ëœë‹¤. 6Â·25ì „ìŸìœ¼ë¡œ ì „êµ­ 106ë§Œ1000í•„ì§€ì˜ ì§€ì ê³µë¶€ê°€ ë¶„Â·ì†Œì‹¤ë˜ê³ , ì•½ 80%ì˜ ì§€ì ì¸¡ëŸ‰ê¸°ì¤€ì ì„
    ìƒì–´ë²„ë¦° ê²ƒë„ í•œ ì›ì¸ì´ë‹¤. í† ì§€ê³µë²•í•™íšŒëŠ” 2005ë…„ ì§€ì ë¶ˆë¶€í•©ì— ë”°ë¥¸ ê²½ê³„ë¶„ìŸìœ¼ë¡œ ì—°ê°„ ì•½ 3800ì–µì›ì˜ ì†Œì†¡ë¹„ìš©ì´ ë°œìƒí•œ ê²ƒìœ¼ë¡œ
    ì¶”ì •í–ˆë‹¤. ë˜ ê²½ê³„í™•ì¸ì¸¡ëŸ‰ìœ¼ë¡œ ì—°ê°„ 900ì–µì›ì˜ ë¹„ìš©ì´ ì§€ì¶œë˜ê³  ìˆë‹¤. ì •ë¶€ëŠ” ì´ 8410ì–µì›ì„ íˆ¬ì…, 2020ë…„ê¹Œì§€ 280ë§Œí•„ì§€ë¥¼,
    ë‚˜ë¨¸ì§€ 274ë§Œí•„ì§€ëŠ” 2030ë…„ê¹Œì§€ ì •ë¹„í•  ê³„íšì´ë‹¤. êµ­í† ë¶€ ê´€ê³„ìëŠ” "ì§€ì ë¶ˆë¶€í•©ì§€ê°€ ì •ë¹„ë˜ë©´ ê²½ê³„ë¶„ìŸì´ í•´ì†Œë¼ ì‚¬íšŒì  ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜
    ìˆê³ , ê°œì¸ì˜ ì¬ì‚°ê¶Œ í–‰ì‚¬ë„ ìˆ˜ì›”í•´ ì§ˆ ê²ƒ"ì´ë¼ê³  ê¸°ëŒ€í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì „êµ­ì— ê±¸ì¹œ ì „ë©´ì ì¸ ì§€ì ì¬ì¡°ì‚¬ê°€ ì•„ë‹ˆë¼ ë¶ˆë¶€í•©ì§€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ
    ë‹¨ê³„ì  ì¶”ì§„ì´ì–´ì„œ í•œê³„ê°€ ìˆë‹¤ëŠ” ì§€ì ì´ë‹¤. ì•ìœ¼ë¡œ ì¬ì¡°ì‚¬ê°€ ì§„í–‰ë˜ë©´ ë¶ˆë¶€í•©ì§€ê°€ ê³„ì† ë‚˜íƒ€ë‚˜ê²Œ ë  ê²ƒì¸ë° ê·¸ ë•Œë§ˆë‹¤ ê²½ê³„ì¡°ì •ì„ í•´ì•¼ í•˜ëŠ”
    ë²ˆê±°ë¡œì›€ì´ ìˆë‹¤ëŠ” ê²ƒ. íŠ¹íˆ ë¶ˆë¶€í•©ì§€ì— ëŒ€í•œ ê²½ê³„ì¡°ì •ì€ ì´í•´ê°€ ì²¨ì˜ˆí•˜ê²Œ ì¶©ëŒí•˜ë‹¤ ë³´ë‹ˆ ì‚¬ì—…ì¶”ì§„ì´ ë§¤ìš° ì–´ë µë‹¤. ì´ ë•Œë¬¸ì— ì „ë©´ì ì¸
    ì¬ì¡°ì‚¬ë¥¼ í†µí•´ í•œ ë²ˆì— ë§ˆë¬´ë¦¬í•˜ëŠ” ê²ƒì´ ìˆ˜ì›”í•˜ë‹¤ëŠ” ì„¤ëª…ì´ë‹¤. ëŒ€í•œì§€ì ê³µì‚¬ ê´€ê³„ìëŠ” "ì˜¤ëœ ì§„í†µ ëì— ì§€ì ì¬ì¡°ì‚¬ì‚¬ì—…ì„ ì¶”ì§„í•˜ê²Œ ë¼
    ê¸°ì˜ë‹¤"ë©´ì„œë„ "ì›ë˜ ì „ë©´ì ì¸ ì‚¬ì—…ì¶”ì§„ì„ ì›í–ˆìœ¼ë‚˜ ì˜ˆì‚° ë“±ì˜ ë¬¸ì œë¡œ ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰í•˜ê²Œ ë¼ ì•„ì‰½ë‹¤"ê³  ë§í–ˆë‹¤.
model-index:
- name: SentenceTransformer
  results:
  - task:
      type: information-retrieval
      name: Information Retrieval
    dataset:
      name: miracl
      type: miracl
    metrics:
    - type: cosine_accuracy@1
      value: 0.6103286384976526
      name: Cosine Accuracy@1
    - type: cosine_accuracy@3
      value: 0.8169014084507042
      name: Cosine Accuracy@3
    - type: cosine_accuracy@5
      value: 0.8732394366197183
      name: Cosine Accuracy@5
    - type: cosine_accuracy@10
      value: 0.92018779342723
      name: Cosine Accuracy@10
    - type: cosine_precision@1
      value: 0.6103286384976526
      name: Cosine Precision@1
    - type: cosine_precision@3
      value: 0.378716744913928
      name: Cosine Precision@3
    - type: cosine_precision@5
      value: 0.27605633802816903
      name: Cosine Precision@5
    - type: cosine_precision@10
      value: 0.17276995305164322
      name: Cosine Precision@10
    - type: cosine_recall@1
      value: 0.3846655691726114
      name: Cosine Recall@1
    - type: cosine_recall@3
      value: 0.5901991071005155
      name: Cosine Recall@3
    - type: cosine_recall@5
      value: 0.6794216477315068
      name: Cosine Recall@5
    - type: cosine_recall@10
      value: 0.7694903427297795
      name: Cosine Recall@10
    - type: cosine_ndcg@10
      value: 0.6833112035481234
      name: Cosine Ndcg@10
    - type: cosine_mrr@10
      value: 0.7262426410313736
      name: Cosine Mrr@10
    - type: cosine_map@100
      value: 0.6073885234240499
      name: Cosine Map@100
    - type: dot_accuracy@1
      value: 0.6103286384976526
      name: Dot Accuracy@1
    - type: dot_accuracy@3
      value: 0.8169014084507042
      name: Dot Accuracy@3
    - type: dot_accuracy@5
      value: 0.8732394366197183
      name: Dot Accuracy@5
    - type: dot_accuracy@10
      value: 0.92018779342723
      name: Dot Accuracy@10
    - type: dot_precision@1
      value: 0.6103286384976526
      name: Dot Precision@1
    - type: dot_precision@3
      value: 0.378716744913928
      name: Dot Precision@3
    - type: dot_precision@5
      value: 0.27605633802816903
      name: Dot Precision@5
    - type: dot_precision@10
      value: 0.17276995305164322
      name: Dot Precision@10
    - type: dot_recall@1
      value: 0.3846655691726114
      name: Dot Recall@1
    - type: dot_recall@3
      value: 0.5901991071005155
      name: Dot Recall@3
    - type: dot_recall@5
      value: 0.6794216477315068
      name: Dot Recall@5
    - type: dot_recall@10
      value: 0.7694903427297795
      name: Dot Recall@10
    - type: dot_ndcg@10
      value: 0.6723275985412543
      name: Dot Ndcg@10
    - type: dot_mrr@10
      value: 0.7262426410313736
      name: Dot Mrr@10
    - type: dot_map@100
      value: 0.6073885234240499
      name: Dot Map@100
license: apache-2.0
base_model:
- BAAI/bge-m3
---


<img src="https://cdn-uploads.huggingface.co/production/uploads/642b0c2fecec03b4464a1d9b/9uN5ypGY-GRGgakLs_s1o.png" width="600">


# SentenceTransformer

This is a [sentence-transformers](https://www.SBERT.net) model trained on the train_set dataset. It maps sentences & paragraphs to a 1024-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

- Learning other languages â€‹â€‹besides Chinese and English is insufficient, so additional learning is needed to optimize use of other languages.
- This model is additionally trained on the Korean dataset.

### Model Description
- **Model Type:** Sentence Transformer
  Transformer Encoder
- **Maximum Sequence Length:** 8192 tokens
- **Output Dimensionality:** 1024 tokens
- **Similarity Function:** Cosine Similarity

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ğŸ¤— Hub
model = SentenceTransformer("dragonkue/bge-m3-ko")
# Run inference
sentences = [
    'ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œ ëŠ¥ë ¥ì´ ì—†ëŠ” ì„ì‚°ë¶€ëŠ” ëª‡ ì¢…ì— í•´ë‹¹í•˜ë‹ˆ?',
    'ë‚´ë…„ë¶€í„° ì €ì†Œë“ì¸µ 1ì„¸ ë¯¸ë§Œ ì•„ë™ì˜ \nì˜ë£Œë¹„ ë¶€ë‹´ì´ ë” ë‚®ì•„ì§„ë‹¤!\nì˜ë£Œê¸‰ì—¬ì œë„ ê°œìš”\nâ–¡ (ëª©ì ) ìƒí™œìœ ì§€ ëŠ¥ë ¥ì´ ì—†ê±°ë‚˜ ìƒí™œì´ ì–´ë ¤ìš´ êµ­ë¯¼ë“¤ì—ê²Œ ë°œìƒí•˜ëŠ” ì§ˆë³‘, ë¶€ìƒ, ì¶œì‚° ë“±ì— ëŒ€í•´ êµ­ê°€ê°€ ì˜ë£Œì„œë¹„ìŠ¤ ì œê³µ\nâ–¡ (ì§€ì›ëŒ€ìƒ) êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì, íƒ€ ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì ë“±\n\n| êµ¬ë¶„ | êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì | êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ë²• ì´ì™¸ì˜ íƒ€ ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì |\n| --- | --- | --- |\n| 1ì¢… | â—‹ êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œëŠ¥ë ¥ì´ ì—†ëŠ” ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê°€êµ¬ - 18ì„¸ ë¯¸ë§Œ, 65ì„¸ ì´ìƒ - 4ê¸‰ ì´ë‚´ ì¥ì• ì¸ - ì„ì‚°ë¶€, ë³‘ì—­ì˜ë¬´ì´í–‰ì ë“± | â—‹ ì´ì¬ë¯¼(ì¬í•´êµ¬í˜¸ë²•) â—‹ ì˜ìƒì ë° ì˜ì‚¬ìì˜ ìœ ì¡±â—‹ êµ­ë‚´ ì…ì–‘ëœ 18ì„¸ ë¯¸ë§Œ ì•„ë™â—‹ êµ­ê°€ìœ ê³µì ë° ê·¸ ìœ ì¡±â€¤ê°€ì¡±â—‹ êµ­ê°€ë¬´í˜•ë¬¸í™”ì¬ ë³´ìœ ì ë° ê·¸ ê°€ì¡±â—‹ ìƒˆí„°ë¯¼(ë¶í•œì´íƒˆì£¼ë¯¼)ê³¼ ê·¸ ê°€ì¡±â—‹ 5â€¤18 ë¯¼ì£¼í™”ìš´ë™ ê´€ë ¨ì ë° ê·¸ ìœ ê°€ì¡±â—‹ ë…¸ìˆ™ì¸ â€» í–‰ë ¤í™˜ì (ì˜ë£Œê¸‰ì—¬ë²• ì‹œí–‰ë ¹) |\n| 2ì¢… | â—‹ êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œëŠ¥ë ¥ì´ ìˆëŠ” ê°€êµ¬ | - |\n',
    'ì´ì–´ ì´ë‚  ì˜¤í›„ 1ì‹œ30ë¶„ë¶€í„° ì—´ë¦´ ì˜ˆì •ì´ë˜ ìŠ¤ë…¸ë³´ë“œ ì—¬ì ìŠ¬ë¡œí”„ìŠ¤íƒ€ì¼ ì˜ˆì„  ê²½ê¸°ëŠ” ì—°ê¸°ë¥¼ ê±°ë“­í•˜ë‹¤ ì·¨ì†Œëë‹¤. ì¡°ì§ìœ„ëŠ” ì˜ˆì„  ì—†ì´ ë‹¤ìŒ ë‚  ê²°ì„ ì—ì„œ ì°¸ê°€ì 27ëª…ì´ í•œë²ˆì— ê²½ê¸°í•´ ìˆœìœ„ë¥¼ ê°€ë¦¬ê¸°ë¡œ í–ˆë‹¤.',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 1024]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

## Evaluation

### Metrics
- ndcg, mrr, map metrics are metrics that consider ranking, while accuracy, precision, and recall are metrics that do not consider ranking. (Example: When considering ranking for retrieval top 10, different scores are given when the correct document is in 1st place and when it is in 10th place. However, accuracy, precision, and recall scores are the same if they are in the top 10.)

#### Information Retrieval
* Korean Embedding Benchmark is a benchmark with a relatively long 3/4 quantile of string length of 1024

##### Korean Embedding Benchmark with AutoRAG

This is a benchmark of Korean embedding models. 
(https://github.com/Marker-Inc-Korea/AutoRAG-example-korean-embedding-benchmark)

- Top-k 1

| Model name                            | F1         | Recall     | Precision  | mAP        | mRR        | NDCG       |
|---------------------------------------|------------|------------|------------|------------|------------|------------|
| paraphrase-multilingual-mpnet-base-v2 | 0.3596     | 0.3596     | 0.3596     | 0.3596     | 0.3596     | 0.3596     |
| KoSimCSE-roberta                      | 0.4298     | 0.4298     | 0.4298     | 0.4298     | 0.4298     | 0.4298     |
| Cohere embed-multilingual-v3.0        | 0.3596     | 0.3596     | 0.3596     | 0.3596     | 0.3596     | 0.3596     |
| openai ada 002                        | 0.4737     | 0.4737     | 0.4737     | 0.4737     | 0.4737     | 0.4737     |
| multilingual-e5-large-instruct        | 0.4649     | 0.4649     | 0.4649     | 0.4649     | 0.4649     | 0.4649     |
| Upstage Embedding                     | 0.6579     | 0.6579     | 0.6579     | 0.6579     | 0.6579     | 0.6579     |
| paraphrase-multilingual-MiniLM-L12-v2 | 0.2982     | 0.2982     | 0.2982     | 0.2982     | 0.2982     | 0.2982     |
| openai_embed_3_small                  | 0.5439     | 0.5439     | 0.5439     | 0.5439     | 0.5439     | 0.5439     |
| ko-sroberta-multitask                 | 0.4211     | 0.4211     | 0.4211     | 0.4211     | 0.4211     | 0.4211     |
| openai_embed_3_large                  | 0.6053     | 0.6053     | 0.6053     | 0.6053     | 0.6053     | 0.6053     |
| KU-HIAI-ONTHEIT-large-v1              | 0.7105     | 0.7105     | 0.7105     | 0.7105     | 0.7105     | 0.7105     |
| KU-HIAI-ONTHEIT-large-v1.1            | 0.7193     | 0.7193     | 0.7193     | 0.7193     | 0.7193     | 0.7193     |
| kf-deberta-multitask                  | 0.4561     | 0.4561     | 0.4561     | 0.4561     | 0.4561     | 0.4561     |
| gte-multilingual-base                 | 0.5877     | 0.5877     | 0.5877     | 0.5877     | 0.5877     | 0.5877     |
| KoE5                                  | 0.7018     | 0.7018     | 0.7018     | 0.7018     | 0.7018     | 0.7018     |
| BGE-m3                                | 0.6578     | 0.6578     | 0.6578     | 0.6578     | 0.6578     | 0.6578     |
| bge-m3-korean                         | 0.5351     | 0.5351     | 0.5351     | 0.5351     | 0.5351     | 0.5351     |
| **BGE-m3-ko**                         | **0.7456** | **0.7456** | **0.7456** | **0.7456** | **0.7456** | **0.7456** |

- Top-k 3

| Model name                            | F1         | Recall     | Precision  | mAP        | mRR        | NDCG       |
|---------------------------------------|------------|------------|------------|------------|------------|------------|
| paraphrase-multilingual-mpnet-base-v2 | 0.2368     | 0.4737     | 0.1579     | 0.2032     | 0.2032     | 0.2712     |
| KoSimCSE-roberta                      | 0.3026     | 0.6053     | 0.2018     | 0.2661     | 0.2661     | 0.3515     |
| Cohere embed-multilingual-v3.0        | 0.2851     | 0.5702     | 0.1901     | 0.2515     | 0.2515     | 0.3321     |
| openai ada 002                        | 0.3553     | 0.7105     | 0.2368     | 0.3202     | 0.3202     | 0.4186     |
| multilingual-e5-large-instruct        | 0.3333     | 0.6667     | 0.2222     | 0.2909     | 0.2909     | 0.3856     |
| Upstage Embedding                     | 0.4211     | 0.8421     | 0.2807     | **0.3509** | **0.3509** | 0.4743     |
| paraphrase-multilingual-MiniLM-L12-v2 | 0.2061     | 0.4123     | 0.1374     | 0.1740     | 0.1740     | 0.2340     |
| openai_embed_3_small                  | 0.3640     | 0.7281     | 0.2427     | 0.3026     | 0.3026     | 0.4097     |
| ko-sroberta-multitask                 | 0.2939     | 0.5877     | 0.1959     | 0.2500     | 0.2500     | 0.3351     |
| openai_embed_3_large                  | 0.3947     | 0.7895     | 0.2632     | 0.3348     | 0.3348     | 0.4491     |
| KU-HIAI-ONTHEIT-large-v1              | 0.4386     | 0.8772     | 0.2924     | 0.3421     | 0.3421     | 0.4766     |
| KU-HIAI-ONTHEIT-large-v1.1            | 0.4430     | 0.8860     | 0.2953     | 0.3406     | 0.3406     | 0.4778     |
| kf-deberta-multitask                  | 0.3158     | 0.6316     | 0.2105     | 0.2792     | 0.2792     | 0.3679     |
| gte-multilingual-base                 | 0.4035     | 0.8070     | 0.2690     | 0.3450     | 0.3450     | 0.4614     |
| KoE5                                  | 0.4254     | 0.8509     | 0.2836     | 0.3173     | 0.3173     | 0.4514     |
| BGE-m3                                | 0.4254     | 0.8508     | 0.2836     | 0.3421     | 0.3421     | 0.4701     |
| bge-m3-korean                         | 0.3684     | 0.7368     | 0.2456     | 0.3143     | 0.3143     | 0.4207     | 
| **BGE-m3-ko**                         | **0.4517** | **0.9035** | **0.3011** | 0.3494     | 0.3494     | **0.4886** |

- Top-k 5

| Model name                            | F1         | Recall     | Precision  | mAP        | mRR        | NDCG       |
|---------------------------------------|------------|------------|------------|------------|------------|------------|
| paraphrase-multilingual-mpnet-base-v2 | 0.1813     | 0.5439     | 0.1088     | 0.1575     | 0.1575     | 0.2491     |
| KoSimCSE-roberta                      | 0.2164     | 0.6491     | 0.1298     | 0.1751     | 0.1751     | 0.2873     |
| Cohere embed-multilingual-v3.0        | 0.2076     | 0.6228     | 0.1246     | 0.1640     | 0.1640     | 0.2731     |
| openai ada 002                        | 0.2602     | 0.7807     | 0.1561     | 0.2139     | 0.2139     | 0.3486     |
| multilingual-e5-large-instruct        | 0.2544     | 0.7632     | 0.1526     | 0.2194     | 0.2194     | 0.3487     |
| Upstage Embedding                     | 0.2982     | 0.8947     | 0.1789     | **0.2237** | **0.2237** | 0.3822     |
| paraphrase-multilingual-MiniLM-L12-v2 | 0.1637     | 0.4912     | 0.0982     | 0.1437     | 0.1437     | 0.2264     |
| openai_embed_3_small                  | 0.2690     | 0.8070     | 0.1614     | 0.2148     | 0.2148     | 0.3553     |
| ko-sroberta-multitask                 | 0.2164     | 0.6491     | 0.1298     | 0.1697     | 0.1697     | 0.2835     |
| openai_embed_3_large                  | 0.2807     | 0.8421     | 0.1684     | 0.2088     | 0.2088     | 0.3586     |
| KU-HIAI-ONTHEIT-large-v1              | 0.3041     | 0.9123     | 0.1825     | 0.2137     | 0.2137     | 0.3783     |
| KU-HIAI-ONTHEIT-large-v1.1            | **0.3099** | **0.9298** | **0.1860** | 0.2148     | 0.2148     | **0.3834** |
| kf-deberta-multitask                  | 0.2281     | 0.6842     | 0.1368     | 0.1724     | 0.1724     | 0.2939     |
| gte-multilingual-base                 | 0.2865     | 0.8596     | 0.1719     | 0.2096     | 0.2096     | 0.3637     |
| KoE5                                  | 0.2982     | 0.8947     | 0.1789     | 0.2054     | 0.2054     | 0.3678     |
| BGE-m3                                | 0.3041     | 0.9123     | 0.1825     | 0.2193     | 0.2193     | 0.3832     |
| bge-m3-korean                         | 0.2661     | 0.7982     | 0.1596     | 0.2116     | 0.2116     | 0.3504     |
| **BGE-m3-ko**                         | **0.3099** | **0.9298** | **0.1860** | 0.2098     | 0.2098     | 0.3793     |

- Top-k 10

| Model name                            | F1         | Recall     | Precision  | mAP        | mRR        | NDCG       |
|---------------------------------------|------------|------------|------------|------------|------------|------------|
| paraphrase-multilingual-mpnet-base-v2 | 0.1212     | 0.6667     | 0.0667     | **0.1197** | **0.1197** | 0.2382     |
| KoSimCSE-roberta                      | 0.1324     | 0.7281     | 0.0728     | 0.1080     | 0.1080     | 0.2411     |
| Cohere embed-multilingual-v3.0        | 0.1324     | 0.7281     | 0.0728     | 0.1150     | 0.1150     | 0.2473     |
| openai ada 002                        | 0.1563     | 0.8596     | 0.0860     | 0.1051     | 0.1051     | 0.2673     |
| multilingual-e5-large-instruct        | 0.1483     | 0.8158     | 0.0816     | 0.0980     | 0.0980     | 0.2520     |
| Upstage Embedding                     | 0.1707     | 0.9386     | 0.0939     | 0.1078     | 0.1078     | 0.2848     |
| paraphrase-multilingual-MiniLM-L12-v2 | 0.1053     | 0.5789     | 0.0579     | 0.0961     | 0.0961     | 0.2006     |
| openai_embed_3_small                  | 0.1547     | 0.8509     | 0.0851     | 0.0984     | 0.0984     | 0.2593     |
| ko-sroberta-multitask                 | 0.1276     | 0.7018     | 0.0702     | 0.0986     | 0.0986     | 0.2275     |
| openai_embed_3_large                  | 0.1643     | 0.9035     | 0.0904     | 0.1180     | 0.1180     | 0.2855     |
| KU-HIAI-ONTHEIT-large-v1              | 0.1707     | 0.9386     | 0.0939     | 0.1105     | 0.1105     | 0.2860     |
| KU-HIAI-ONTHEIT-large-v1.1            | 0.1722     | 0.9474     | 0.0947     | 0.1033     | 0.1033     | 0.2822     |
| kf-deberta-multitask                  | 0.1388     | 0.7632     | 0.0763     | 0.1        | 0.1        | 0.2422     |
| gte-multilingual-base                 | 0.1675     | 0.9211     | 0.0921     | 0.1066     | 0.1066     | 0.2805     |
| KoE5                                  | 0.1675     | 0.9211     | 0.0921     | 0.1011     | 0.1011     | 0.2750     |
| BGE-m3                                | 0.1707     | 0.9386     | 0.0939     | 0.1130     | 0.1130     | 0.2884     |
| bge-m3-korean                         | 0.1579     | 0.8684     | 0.0868     | 0.1093     | 0.1093     | 0.2721     |
| **BGE-m3-ko**                         | **0.1770** | **0.9736** | **0.0974** | 0.1097     | 0.1097     | **0.2932** |



#### Information Retrieval
* Dataset: `miracl-ko` (https://github.com/project-miracl/miracl)
* miracl benchmark is a benchmark with a relatively short 3/4 quantile of string length of 220 on the Korean Wikidata set.
* Evaluated with [<code>InformationRetrievalEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.InformationRetrievalEvaluator)

| Metric              | Value      |
|:--------------------|:-----------|
| cosine_accuracy@1   | 0.6103     |
| cosine_accuracy@3   | 0.8169     |
| cosine_accuracy@5   | 0.8732     |
| cosine_accuracy@10  | 0.9202     |
| cosine_precision@1  | 0.6103     |
| cosine_precision@3  | 0.3787     |
| cosine_precision@5  | 0.2761     |
| cosine_precision@10 | 0.1728     |
| cosine_recall@1     | 0.3847     |
| cosine_recall@3     | 0.5902     |
| cosine_recall@5     | 0.6794     |
| cosine_recall@10    | 0.7695     |
| **cosine_ndcg@10**  | **0.6833** |
| cosine_mrr@10       | 0.7262     |
| cosine_map@100      | 0.6074     |
| dot_accuracy@1      | 0.6103     |
| dot_accuracy@3      | 0.8169     |
| dot_accuracy@5      | 0.8732     |
| dot_accuracy@10     | 0.9202     |
| dot_precision@1     | 0.6103     |
| dot_precision@3     | 0.3787     |
| dot_precision@5     | 0.2761     |
| dot_precision@10    | 0.1728     |
| dot_recall@1        | 0.3847     |
| dot_recall@3        | 0.5902     |
| dot_recall@5        | 0.6794     |
| dot_recall@10       | 0.7695     |
| dot_ndcg@10         | 0.6723     |
| dot_mrr@10          | 0.7262     |
| dot_map@100         | 0.6074     |

## Bias, Risks and Limitations

- Since the evaluation results are different for each domain, it is necessary to compare and evaluate the model in your own domain. In the Miracl benchmark, the evaluation was conducted using the Korean Wikipedia as a corpus, and in this case, the cosine_ndcg@10 score dropped by 0.02 points after learning. However, in the Auto-RAG benchmark, which is a financial domain, the ndcg score increased by 0.09 when it was top 1. This model may be advantageous for use in a specific domain.
- Also, since the miracl benchmark consists of a corpus of relatively short strings, while the Korean Embedding Benchmark consists of a corpus of longer strings, this model may be more advantageous if the length of the corpus you want to use is long.


### Training Hyperparameters
#### Non-Default Hyperparameters
The batch size was referenced from the following paper: Text Embeddings by Weakly-Supervised Contrastive Pre-training (https://arxiv.org/pdf/2212.03533)

- `eval_strategy`: steps
- `per_device_train_batch_size`: 32768
- `per_device_eval_batch_size`: 32768
- `learning_rate`: 3e-05
- `warmup_ratio`: 0.03333333333333333
- `fp16`: True
- `batch_sampler`: no_duplicates

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: steps
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 32768
- `per_device_eval_batch_size`: 32768
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `learning_rate`: 3e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1.0
- `num_train_epochs`: 3
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.03333333333333333
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: True
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: True
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: False
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `batch_sampler`: no_duplicates
- `multi_dataset_batch_sampler`: proportional

</details>


## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```
```bibtex
@misc{bge-m3,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
```bibtex
@article{wang2022text,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->