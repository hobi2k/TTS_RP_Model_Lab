"""
sft_trainer_translator.py

ëª©ì :
- Qwen/Qwen3-1.7B-Base ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ
- ë²ˆì—­ SFT ë°ì´í„°(instruction/input/output JSONL)ë¥¼ ì‚¬ìš©í•´
- LoRA ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •(SFT)í•œë‹¤.

í•µì‹¬ ì„¤ê³„:
1) "ë²ˆì—­ê¸°"ë¡œë§Œ ë™ìž‘í•˜ë„ë¡, ë°ì´í„°ëŠ” instruction+input -> output í˜•íƒœë¡œ í•™ìŠµí•œë‹¤.
2) LoRAë¥¼ ì‚¬ìš©í•´ VRAM ë¶€ë‹´ì„ ì¤„ì´ê³ , ìºë¦­í„°/RP ëª¨ë¸ê³¼ ì—­í• ì„ ë¶„ë¦¬í•œë‹¤.
3) __file__ ê¸°ì¤€ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ uv run / ìž‘ì—… ë””ë ‰í„°ë¦¬ ë³€í™”ì— ì•ˆì „í•˜ê²Œ ë§Œë“ ë‹¤.

uv run models/qwen3_core/sft_trainer_translator.py \
  --bf16 \
  --gradient_checkpointing \
  --max_length 512 \
  --per_device_train_batch_size 8 \
  --gradient_accumulation_steps 4 \
  --num_train_epochs 3 \
  --learning_rate 2e-4
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

import torch
from datasets import load_dataset
from dataclasses import dataclass
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

# ê²½ë¡œ / ê¸°ë³¸ê°’ ì„¤ì •

# ì´ íŒŒì¼ ìœ„ì¹˜: models/qwen3_core/sft_trainer_translator.py
QWEN3_CORE_DIR = Path(__file__).resolve().parent          # .../models/qwen3_core
MODEL_ASSETS_DIR = QWEN3_CORE_DIR / "model_assets"

# ë²ˆì—­ìš© Base ëª¨ë¸ (ë¡œì»¬ ê³ ì •)
DEFAULT_BASE_MODEL_DIR = MODEL_ASSETS_DIR / "qwen3-1.7b-base"

# í•™ìŠµ ë°ì´í„° (WSL ê¸°ì¤€ ì ˆëŒ€ê²½ë¡œ ê¶Œìž¥)
DATA_DIR = Path("/mnt/d/rp_data/singleturn")
DEFAULT_DATA_PATH = DATA_DIR / "ko-ja_translation_sft.jsonl"

# LoRA ì¶œë ¥ ë””ë ‰í„°ë¦¬
DEFAULT_OUTPUT_DIR = MODEL_ASSETS_DIR / "qwen3_1.7_ko2ja_lora"


# í”„ë¡¬í”„íŠ¸ êµ¬ì„±

def build_prompt(instruction: str, user_input: str) -> str:
    """
    ë²ˆì—­ SFTìš© í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•œë‹¤.
    (Chat template ì‚¬ìš© x)

    êµ¬ì¡°:
    ### Instruction:
    ...
    ### Input:
    ...
    ### Output:
    """
    instruction = (instruction or "").strip()
    user_input = (user_input or "").strip()

    return (
        "### Instruction:\n"
        f"{instruction}\n\n"
        "### Input:\n"
        f"{user_input}\n\n"
        "### Output:\n"
    )


def build_full_text(example: Dict[str, str], eos_token: str) -> str:
    """
    í•™ìŠµìš© ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±:
    [Instruction + Input + Output + EOS]
    """
    prompt = build_prompt(example.get("instruction", ""), example.get("input", ""))
    answer = (example.get("output", "") or "").strip()
    return prompt + answer + eos_token


# Tokenize

def tokenize_function(
    examples: Dict[str, List[str]],
    tokenizer: AutoTokenizer,
    max_length: int,
) -> Dict[str, List[List[int]]]:

    eos = tokenizer.eos_token or ""

    texts: List[str] = []
    output_start_positions: List[int] = []

    for inst, inp, out in zip(
        examples["instruction"],
        examples["input"],
        examples["output"],
    ):
        prompt = build_prompt(inst, inp)
        full_text = prompt + (out or "").strip() + eos
        texts.append(full_text)

        # ðŸ”¥ Output ì‹œìž‘ ìœ„ì¹˜ ê³„ì‚° (í† í° ë‹¨ìœ„)
        prompt_ids = tokenizer(
            prompt,
            add_special_tokens=False,
        )["input_ids"]
        output_start_positions.append(len(prompt_ids))

    tokenized = tokenizer(
        texts,
        max_length=max_length,
        truncation=True,
        padding=False,
        return_attention_mask=True,
    )

    labels = []

    for input_ids, output_start in zip(
        tokenized["input_ids"],
        output_start_positions,
    ):
        # Output ì´ì „ì€ ì „ë¶€ -100
        label = [-100] * output_start + input_ids[output_start:]
        labels.append(label)

    tokenized["labels"] = labels
    return tokenized


# LoRA ì„¤ì •

def build_lora_config(r: int, alpha: int, dropout: float) -> LoraConfig:
    """
    Qwen ê³„ì—´ ë²ˆì—­ SFTì— ì í•©í•œ LoRA ì„¤ì •
    """
    return LoraConfig(
        r=r,
        lora_alpha=alpha,
        lora_dropout=dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
    )

@dataclass
class DataCollatorForCausalLM:
    tokenizer: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # 1) labelsë¥¼ ë¶„ë¦¬ (tokenizer.padì— ë„˜ê¸°ì§€ ì•ŠìŒ)
        labels = [f.pop("labels") for f in features]

        # 2) input_ids / attention_maskë§Œ padding
        batch = self.tokenizer.pad(
            features,
            padding=True,
            return_tensors="pt",
        )

        # 3) labels ìˆ˜ë™ padding
        max_len = batch["input_ids"].size(1)
        padded_labels = []

        for label in labels:
            pad_len = max_len - len(label)
            padded = label + [self.tokenizer.pad_token_id] * pad_len
            padded_labels.append(padded)

        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)

        # 4) pad tokenì€ loss ê³„ì‚°ì—ì„œ ì œì™¸
        labels_tensor[labels_tensor == self.tokenizer.pad_token_id] = -100

        batch["labels"] = labels_tensor
        return batch

# Main

def main() -> None:
    parser = argparse.ArgumentParser()

    # ----- ê²½ë¡œ -----
    parser.add_argument(
        "--model_name",
        type=str,
        default=str(DEFAULT_BASE_MODEL_DIR),
        help="Local path to base Qwen3 model (model_assets/qwen3-1.7b-base)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=str(DEFAULT_DATA_PATH),
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=str(DEFAULT_OUTPUT_DIR),
    )

    # ----- í•™ìŠµ íŒŒë¼ë¯¸í„° -----
    parser.add_argument("--max_length", type=int, default=512)
    parser.add_argument("--num_train_epochs", type=float, default=5.0)
    parser.add_argument("--per_device_train_batch_size", type=int, default=8)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--warmup_ratio", type=float, default=0.03)
    parser.add_argument("--weight_decay", type=float, default=0.0)
    parser.add_argument("--logging_steps", type=int, default=20)
    parser.add_argument("--save_steps", type=int, default=500)
    parser.add_argument("--save_total_limit", type=int, default=2)

    # ----- LoRA -----
    parser.add_argument("--lora_r", type=int, default=32)
    parser.add_argument("--lora_alpha", type=int, default=64)
    parser.add_argument("--lora_dropout", type=float, default=0.05)

    # ----- ê¸°íƒ€ -----
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--bf16", action="store_true")
    parser.add_argument("--fp16", action="store_true")
    parser.add_argument("--gradient_checkpointing", action="store_true")
    parser.add_argument("--eval_split", type=float, default=0.02)

    args = parser.parse_args()

    data_path = Path(args.data_path)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Tokenizer / Model ë¡œë“œ (ë¡œì»¬)

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.bfloat16 if args.bf16 else (torch.float16 if args.fp16 else None),
        device_map="auto",
    )

    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        model.config.use_cache = False

    # LoRA ì ìš©

    lora_config = build_lora_config(
        r=args.lora_r,
        alpha=args.lora_alpha,
        dropout=args.lora_dropout,
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # Dataset ë¡œë“œ

    raw = load_dataset("json", data_files=str(data_path))
    dataset = raw["train"]

    if args.eval_split > 0:
        split = dataset.train_test_split(
            test_size=args.eval_split,
            seed=args.seed,
        )
        train_ds = split["train"]
        eval_ds = split["test"]
    else:
        train_ds = dataset
        eval_ds = None

    train_ds = train_ds.map(
        lambda batch: tokenize_function(batch, tokenizer, args.max_length),
        batched=True,
        remove_columns=train_ds.column_names,
        desc="Tokenizing train dataset",
    )

    if eval_ds is not None:
        eval_ds = eval_ds.map(
            lambda batch: tokenize_function(batch, tokenizer, args.max_length),
            batched=True,
            remove_columns=eval_ds.column_names,
            desc="Tokenizing eval dataset",
        )

    # Trainer


    data_collator = DataCollatorForCausalLM(tokenizer=tokenizer)

    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_ratio=args.warmup_ratio,
        weight_decay=args.weight_decay,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        eval_strategy="steps" if eval_ds is not None else "no",
        eval_steps=args.save_steps if eval_ds is not None else None,
        bf16=args.bf16,
        fp16=args.fp16,
        report_to="none",
        seed=args.seed,
        dataloader_pin_memory=True,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    trainer.train()

    # ì €ìž¥ (LoRAë§Œ)

    trainer.model.save_pretrained(output_dir / "lora_adapter")
    tokenizer.save_pretrained(output_dir / "tokenizer")

    with (output_dir / "train_args.json").open("w", encoding="utf-8") as f:
        json.dump(vars(args), f, ensure_ascii=False, indent=2)

    print(f"[DONE] LoRA adapter saved to: {output_dir / 'lora_adapter'}")


if __name__ == "__main__":
    main()